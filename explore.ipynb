{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to run tasks in parallel\n",
    "from gevent import monkey\n",
    "monkey.patch_all()\n",
    "import gevent\n",
    "from gevent.queue import Queue\n",
    "import itertools\n",
    "from typing import Callable\n",
    "from polygon import RESTClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# represents a nil value for a float\n",
    "nil_float = -9999.0\n",
    "\n",
    "def polygon_factory():\n",
    "    return RESTClient(api_key=os.getenv(\"POLYGON_KEY\"))\n",
    "\n",
    "# makes one client for one-off calls\n",
    "client = polygon_factory()\n",
    "\n",
    "# Function to process a queue with a client\n",
    "def process_queue(client:RESTClient, queue:Queue, fn:Callable[[dict, RESTClient], None]):\n",
    "    while not queue.empty():\n",
    "        item = queue.get()\n",
    "        fn(item, client)\n",
    "\n",
    "# Pass the dict with all items, fn operates on each item and add fields where needed\n",
    "def run_on_all_items(items:list[dict], fn:Callable[[dict, RESTClient], None], cli_gen:Callable[[], RESTClient],  N = 8):\n",
    "    clients = [cli_gen() for _ in range(N)]\n",
    "    queues = [Queue() for _ in range(N)]\n",
    "\n",
    "    # Distribute items into the queues\n",
    "    for item, queue in zip(items, itertools.cycle(queues)):\n",
    "        queue.put_nowait(item)\n",
    "\n",
    "    # Create greenlets for each client and queue, and process them\n",
    "    greenlets = [\n",
    "        gevent.spawn(process_queue, client, queue, fn)\n",
    "        for client, queue in zip(clients, queues)\n",
    "    ]\n",
    "\n",
    "    # Wait for all greenlets to complete\n",
    "    gevent.joinall(greenlets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all exchanges\n",
    "exchange_list = client.get_exchanges(asset_class='stocks')\n",
    "\n",
    "for t in exchange_list:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygon.rest.models import Ticker\n",
    "from typing import Any, Dict\n",
    "\n",
    "# every ticker we hold here will be TIKCER: {<props...>}\n",
    "ticker_dict: Dict[str, Any] = {}\n",
    "\n",
    "exchanges_we_care = [\n",
    "    \"XASE\", # 234 https://www.tradinghours.com/mic/s/xase ?\n",
    "    \"XNAS\", # NASDAQ 3370\n",
    "    \"XNYS\", # NYSE 1797\n",
    "]\n",
    "\n",
    "def list_all_tickers_from_exchange(mic):\n",
    "    tickers = client.list_tickers(market=\"stocks\", limit=1000, exchange=mic, type=\"CS\")\n",
    "    # print(f\"--{mic}:{len(tickers)}\")\n",
    "    return tickers\n",
    "\n",
    "for ex in exchanges_we_care:\n",
    "    # print(f\"exchange: {ex}\")\n",
    "    l = 0\n",
    "    tickers_itr = list_all_tickers_from_exchange(ex)\n",
    "    t: Ticker = None\n",
    "    for t in tickers_itr:\n",
    "        l += 1\n",
    "        if t.ticker in ticker_dict:\n",
    "            print(f\"DUPLICATE: {ex}:{t.ticker} <> {ticker_dict[t.ticker]['mic']}:{t.ticker}\")\n",
    "        ticker_dict[t.ticker] = {\n",
    "            \"t\": t.ticker,\n",
    "            \"mic\": ex,\n",
    "            \"name\": t.name,\n",
    "        }\n",
    "        # it's all USD for we got so far\n",
    "        if t.currency_name != 'usd':\n",
    "            print(f\"-- {t.ticker} currency: {t.currency_name}\")\n",
    "            ticker_dict[t.ticker][\"curr\"] = t.currency_name\n",
    "    print(f\"exchange: {ex} tickers: {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ Sample Down?\n",
    "\n",
    "Option to sample down here and make the code go hundreds of time faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'META MSFT AMZN GOOGL WMT INTC NET SQ COIN MTCH QS'\n",
    "\n",
    "ticker_sample = {}\n",
    "for t in sample.split(' '):\n",
    "    ticker_sample[t] = ticker_dict[t]\n",
    "\n",
    "# RUN ALL STOCKS\n",
    "ticker_sample = ticker_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have collected all tickers in `ticker_dict`  \n",
    "\n",
    "Let's collect some **financial metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 11m  -> N=8 1.24m\n",
    "\n",
    "from polygon.rest.models import TickerDetails\n",
    "\n",
    "def add_details(t_obj:dict, client:RESTClient):\n",
    "    t = t_obj['t']\n",
    "    # https://polygon.io/docs/stocks/get_v3_reference_tickers__ticker\n",
    "    deets:TickerDetails = client.get_ticker_details(t)\n",
    "    # Whether or not the asset is actively traded. False means the asset has been delisted.\n",
    "    t_obj['active'] = deets.active\n",
    "    # The most recent close price of the ticker multiplied by weighted outstanding shares.\n",
    "    t_obj['market_cap'] = deets.market_cap\n",
    "    # The shares outstanding calculated assuming all shares of other share classes are converted to this share class.\n",
    "    t_obj['share_count'] = deets.weighted_shares_outstanding\n",
    "    t_obj['employees'] = deets.total_employees\n",
    "    t_obj['sic_description'] = deets.sic_description\n",
    "\n",
    "run_on_all_items(ticker_sample.values(), add_details, polygon_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygon.rest.models import DailyOpenCloseAgg\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def recursive_dict(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: recursive_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursive_dict(elem) for elem in obj]\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        return recursive_dict(obj.__dict__)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# get the last open/close, avoid weekends and empty dates.\n",
    "# ps: would be nice to have a cache for dates, so we don't end up repeating failures\n",
    "def get_last_open_day_data(date_s:str, ticker:str, back: int = 0, halt: int = 5) -> DailyOpenCloseAgg:\n",
    "    if halt < 1:\n",
    "        print(\"halting, too many tries\", ticker, date_s)\n",
    "        # raise RuntimeError(\"too many tries\")\n",
    "        return None\n",
    "    # cast to datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    date = datetime.strptime(date_s, '%Y-%m-%d') - timedelta(days=back)\n",
    "    # get last open day that is not a weekend\n",
    "    while date.weekday() > 4:\n",
    "        date -= timedelta(days=1)\n",
    "    date_s = date.strftime('%Y-%m-%d')\n",
    "    # even if not weekend may still be holiday, and data won't be found\n",
    "    try:\n",
    "        # print(f\"get_last_open_day_data trying: ${ticker} {date_s}\")\n",
    "        day_data = client.get_daily_open_close_agg(ticker=ticker, adjusted=True, date=date_s)\n",
    "        return day_data\n",
    "    except:\n",
    "        return get_last_open_day_data(date_s, ticker, back + 1, halt -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P/E ratio\n",
    "\n",
    "The PE ratio is calculated by dividing the market value price per share by the company's earnings per share. A high P/E ratio can mean that a stock's price is high relative to earnings and possibly overvalued. A low P/E ratio might indicate that the current stock price is low relative to earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 24m -> N=8 4.4m\n",
    "\n",
    "# set: last_price, eps, pe\n",
    "def get_pe(t_obj:dict, client:RESTClient):\n",
    "    ticker = t_obj['t']\n",
    "    t_obj[\"last_price\"] = nil_float\n",
    "    t_obj[\"eps\"] = nil_float\n",
    "    t_obj[\"pe\"] = nil_float\n",
    "    \n",
    "    fundamentals_req = client.vx.list_stock_financials(ticker=ticker, limit=100, timeframe=\"quarterly\")\n",
    "    fundamentals_res = []\n",
    "\n",
    "    for f in fundamentals_req:\n",
    "        fundamentals_res.append(f)\n",
    "\n",
    "    # (IMO) using annual is too terrible outdated, better sum off last 4 quarters\n",
    "    eps = 0\n",
    "    i=0\n",
    "    eps_list = []\n",
    "    list_len = len(fundamentals_res)\n",
    "    while i < 4 and i < list_len:\n",
    "        fundamentals_res[i]\n",
    "        if fundamentals_res[i].financials.income_statement != None and fundamentals_res[i].financials.income_statement.basic_earnings_per_share != None:\n",
    "            eps_list.append(fundamentals_res[i].financials.income_statement.basic_earnings_per_share.value)\n",
    "        i += 1\n",
    "\n",
    "    # note: when i use todays, it gets pretty close to google/yahoo (but not same): https://ca.finance.yahoo.com/quote/META?p=META -- not immediate important, since i'm not looking for precise fundamentals, but it's an important point to fix later.\n",
    "    date = datetime.today().strftime('%Y-%m-%d')\n",
    "    day_data = get_last_open_day_data(date, ticker)\n",
    "    if day_data == None:\n",
    "        return\n",
    "    last_price = day_data.open\n",
    "    if day_data.close != None:\n",
    "        last_price = day_data.close\n",
    "    if last_price == None or last_price == 0:\n",
    "        return\n",
    "    t_obj[\"last_price\"] = last_price\n",
    "\n",
    "    if len(eps_list) < 1:\n",
    "        return\n",
    "\n",
    "    # pro-rate if needed\n",
    "    eps = sum(eps_list) * 4/len(eps_list)\n",
    "\n",
    "    if eps == 0:\n",
    "        return\n",
    "    # print(f\"day_data: {day_data}\")\n",
    "    pe = last_price / eps\n",
    "    \n",
    "    # print(f\"\\nday_data.close:{day_data.close} \\neps:{eps} \\nP/E ={pe}\")\n",
    "    # print(json.dumps(recursive_dict(f0), indent=4, sort_keys=True))\n",
    "    t_obj[\"eps\"] = eps\n",
    "    t_obj[\"pe\"] = pe\n",
    "    return\n",
    "\n",
    "run_on_all_items(ticker_sample.values(), get_pe, polygon_factory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividends\n",
    "\n",
    "Project forward - Trailing Annual Dividend Rate\n",
    "\n",
    "get the last dividend that was paid as \"dividend_type\": \"CD\" in the last year\n",
    "feequencies observed: 1, 2, 4(most common), 12\n",
    "\n",
    "for many reasons, may return none.\n",
    "\n",
    "[Dividends are kinda complicated](https://www.investopedia.com/ask/answers/102714/how-and-when-are-stock-dividends-paid-out.asp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time: 8m  -> N=8  1m\n",
    "from polygon.rest.models import Dividend\n",
    "\n",
    "# eg: https://ca.finance.yahoo.com/quote/MO/key-statistics?p=MO#:~:text=17.56M-,Dividends%20%26%20Splits,-Forward%20Annual%20Dividend\n",
    "# eg code (other API): https://medium.com/swlh/finding-high-dividend-stocks-with-python-c28e02c14e14\n",
    "\n",
    "# set: div_year, div_pc\n",
    "def get_dividends(t_obj:dict, client:RESTClient):\n",
    "    ticker = t_obj['t']\n",
    "    last_price = t_obj['last_price']\n",
    "    t_obj[\"div_year\"] = nil_float\n",
    "    t_obj[\"div_pc\"] = nil_float\n",
    "    t_obj[\"div_freq\"] = nil_float\n",
    "    \n",
    "    # give a little extra in case now is near that time\n",
    "    start_date = datetime.today() - timedelta(days=366 * 1.3)\n",
    "\n",
    "    dividends = client.list_dividends(ticker=ticker, limit=1000, dividend_type=\"CD\", ex_dividend_date_gt=start_date.strftime('%Y-%m-%d'))\n",
    "    dividends_list = []\n",
    "    freq = None\n",
    "    for d in dividends:\n",
    "        d: Dividend = d\n",
    "        # avoid handling different frequencies - keep calculations simple for now\n",
    "        if freq == None:\n",
    "            freq = d.frequency\n",
    "        else:\n",
    "            if freq != d.frequency:\n",
    "                print(f\"div freq changed: {ticker}\")\n",
    "                return\n",
    "        dividends_list.append(d)\n",
    "    # print('LEN:', len(dividends_list))\n",
    "    if len(dividends_list) == 0:\n",
    "        return\n",
    "\n",
    "    freq = dividends_list[0].frequency\n",
    "    t_obj[\"div_freq\"] = freq\n",
    "\n",
    "    if len(dividends_list) < freq:\n",
    "        print(f\"not enough dividends: {len(dividends_list)} - {ticker}\")\n",
    "        return\n",
    "\n",
    "    if freq not in (1, 2, 4, 12):\n",
    "        print(f\"div freq not supported: {freq} - {ticker}\")\n",
    "        return\n",
    "\n",
    "    # make sure the last div is not too old (proportional to frequency)\n",
    "    last_div_date = datetime.strptime(dividends_list[0].ex_dividend_date, '%Y-%m-%d')\n",
    "    if last_div_date < datetime.today() - timedelta(days=((366/freq)+31)):\n",
    "        print(f\"last div too old: {last_div_date} - {ticker} (freq:{freq})\")\n",
    "        print(dividends_list)\n",
    "        return\n",
    "\n",
    "    i=0\n",
    "    vals = []\n",
    "    list_len = len(dividends_list)\n",
    "    while i < freq and i < list_len:\n",
    "        dividends_list[i]\n",
    "        vals.append(dividends_list[i].cash_amount)\n",
    "        i += 1\n",
    "    # print(f\"vals: {vals}\")\n",
    "    if len(vals) == 0:\n",
    "        return\n",
    "    div_year = sum(vals)\n",
    "    # print(f\"dividends ~year: {div_year}\")\n",
    "    div_pc = round(div_year/last_price * 100, 3)\n",
    "    # print(\"dividends %\", div_pc)\n",
    "    t_obj[\"div_year\"] = div_year\n",
    "    t_obj[\"div_pc\"] = div_pc\n",
    "    return\n",
    "\n",
    "# run_on_all_items(ticker_sample.values(), get_dividends, polygon_factory)\n",
    "\n",
    "# debug\n",
    "# get_dividends(ticker_sample['HBI'], client) # -- too old last div: 2022-11-21 00:00:00 - HBI\n",
    "# ticker_sample['HBI']\n",
    "# get_dividends(ticker_sample['BDL'], client) # freq:1\n",
    "# ticker_sample['BDL']\n",
    "\n",
    "# get_dividends(ticker_sample['EVBN'], client) # freq:2\n",
    "# ticker_sample['EVBN']\n",
    "\n",
    "# get_dividends(ticker_sample['AGNC'], client) # freq:12\n",
    "# ticker_sample['AGNC']\n",
    "\n",
    "# get_dividends(ticker_sample['BCPC'], client) # freq:1\n",
    "# ticker_sample['BCPC']\n",
    "\n",
    "# get_dividends(ticker_sample['ADTN'], client) -- missing last quarter (skip)\n",
    "# ticker_sample['ADTN']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle / from pickle\n",
    "import pickle\n",
    "\n",
    "# save\n",
    "# with open('jars/tickers-2023-12-28.pickle', 'wb') as handle:\n",
    "#     pickle.dump(ticker_sample, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # load\n",
    "# with open('jars/tickers-2023-12-28.pickle', 'rb') as handle:\n",
    "#     ticker_sample = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put in a pandas dataframe\n",
    "import pandas as pd\n",
    "# configure number of lines to display in pandas dataframe\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.min_rows\", 30)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "# df = pd.DataFrame([t.__dict__ for t in ticker_sample_list])\n",
    "rows = 'name market_cap share_count employees sic_description last_price eps pe div_pc'.split(' ')\n",
    "df = pd.DataFrame.from_dict(ticker_sample, orient='index', columns=rows)\n",
    "df.index.name = 'ticker'\n",
    "\n",
    "# filter out ALL values that are NaN\n",
    "df = df.dropna()\n",
    "# filter out all data that is less than 9999\n",
    "df = df[(df['pe'] > -100)]\n",
    "df = df[df['eps'] > -100]\n",
    "df = df[df['market_cap'] > 1e9]\n",
    "# df = df[(df['div_pc'] < 50) & (df['div_pc'] > 0)]\n",
    "df = df.sort_values(by=['market_cap'], ascending=False)\n",
    "\n",
    "print(\"len: \", df.shape[0])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook-scaffold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
